{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Implementing the BSBI Algorithm for Inverted Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## importing libraries\n",
    "Let's first import libraries we might need. we will use some libraries as typing checking, and some libraries like string just for punctuation. I use os and json libraries for reading files. And also, as it is said in Problem file, we can use nltk library for tokenizing. I implement two way for tokenizing, one using nltk and one without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:32.853187Z",
     "end_time": "2023-11-23T13:07:33.376353Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import string\n",
    "from typing import List\n",
    "\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Utils functions\n",
    "Now let's define some utils functions that are used in gamma coding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:33.376216Z",
     "end_time": "2023-11-23T13:07:33.376612Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def dec_to_bin(dec_num: int):\n",
    "    \"\"\"\n",
    "    This function converts a decimal number to binary number\n",
    "    \"\"\"\n",
    "    binary_string = bin(dec_num)[2:]\n",
    "    return binary_string\n",
    "\n",
    "\n",
    "def bin_to_dec(binary_string: str):\n",
    "    \"\"\"\n",
    "    This function converts a binary number to decimal number\n",
    "    \"\"\"\n",
    "    if not all(bit in '01' for bit in binary_string):\n",
    "        raise ValueError(\"Input must be a binary string\")\n",
    "\n",
    "    decimal_number = int(binary_string, 2)\n",
    "    return decimal_number\n",
    "\n",
    "\n",
    "def dec_to_gamma(dec_num: int):\n",
    "    \"\"\"\n",
    "    This function convert a decimal number to gamma code.\n",
    "    :param dec_num: int\n",
    "        The number you want to encode\n",
    "    :return:\n",
    "        return Gamma code of the number\n",
    "    \"\"\"\n",
    "    bin_num = dec_to_bin(dec_num)\n",
    "    n = len(bin_num)\n",
    "    unary = (n) * '1'\n",
    "    binary = bin_num[1:]\n",
    "    gamma_code = unary + '0' + binary\n",
    "    return gamma_code\n",
    "\n",
    "\n",
    "def list_to_gamma_code(ls):\n",
    "    \"\"\"\n",
    "    This function convert list of differences of numbers to gamma code using dec_to_gamma() function.\n",
    "    :param ls:\n",
    "        The list you want to encode\n",
    "    :return:\n",
    "        Gamma code of the list\n",
    "    \"\"\"\n",
    "    curr_diff = ls[0]\n",
    "    result = dec_to_gamma(curr_diff)\n",
    "    for i in range(1, len(ls)):\n",
    "        curr_diff = ls[i] - ls[i - 1]\n",
    "        result += dec_to_gamma(curr_diff)\n",
    "    return result\n",
    "\n",
    "\n",
    "def gamma_code_to_list(gamma_code_list):\n",
    "    \"\"\"\n",
    "    This function decoded a gamma code of a list. In this function, we simply loop over the gamma code, and count 1s until\n",
    "    reaching a 0. Then, we decode the substring of the gamma code such that the beginning of the string is the index that\n",
    "    we reach when counting 1s, here it's i, and the end of the substring which is i+counted 1s.\n",
    "    :param gamma_code_list:\n",
    "        The gamma code of a list\n",
    "    :return:\n",
    "        Return the decoded list\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(gamma_code_list):\n",
    "        j = 0\n",
    "        while gamma_code_list[i] == '1':\n",
    "            j += 1\n",
    "            i += 1\n",
    "        dec_num = bin_to_dec('1' + gamma_code_list[i + 1: i + j])\n",
    "        result.append(dec_num)\n",
    "        i += j\n",
    "    for i in range(1, len(result)):\n",
    "        result[i] = result[i - 1] + result[i]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "Let's define our preprocess functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by tokenizing, converting to lowercase, removing punctuation,\n",
    "    and filtering out stop words.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize text\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    # Filter out stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in nltk.corpus.stop_words]\n",
    "    return filtered_tokens\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:33.376339Z",
     "end_time": "2023-11-23T13:07:33.376756Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Main classes\n",
    "Now let's define our classes. I define two classes for this project. Token and Information Retrieval System. I use Token class inside of IR System.\n",
    "### Algorithm\n",
    "Befor going to implementation, let's summarize our algorithm and approach:\n",
    "First of all, we get a list of documents_paths in format of string. We split this documents_paths with batch size, and when looping over documents, we read some of documents and make their posting list then save this posting list in disk and next we go to the next batch of documents and so on. we need to tokenize all these documents. we can split all words using whitespace. Then, we need to get rid of punctuation. This will be done using nltk. at last, it's better to lower case all words so we can index for example \"another\" and \"Another\" in same. After tokenizing, we have to create our posting_list, which is the core of this project. To do this, we should loop over all documents, then inside this loop, we loop over all the tokens that are in the current document. Then we check if the length of posting_list is zero, then we add this token as first word. else if the length of posting_list is more than 0, we find the correct index of the token in posting_list alphabetically. then we check if this token, has been already in posting_list, we just add the current document index in tokens.docs, else, we add this token in the posting_list, then add the current document index. Also we store index of occurence of the token in the relevant documents. And when all documents have been indexed and the batch posting lists are saved, we read them from disk and merge them into main posting list.\n",
    "Now we create our posting list, we can easily query. for intersecting(AND) 2 words, we just need to return document's index of our 2 words that are occurred in same documents. for union(OR), we gather index of word1 with word2. for NOT, we just need to return all document's index that are not in the list of documents that contain our word. And at last, for near, we just need to find documents that either each of these word has been occurred near by at most k words on left or right. we have to cases:\n",
    " * 1)the first_word occurred before second_word.\n",
    " * 2)second_word occurred before first_word.\n",
    "\n",
    "so we loop over a tuple (p1, p2) to cover ordered occurrence of each word. then for each word we loop over its documents, and in each document, we loop over index of that the token is occurred. then if other word(let's say second) is in documents[word1_idx:word1_idx+length], we add the current document index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Token class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:33.376464Z",
     "end_time": "2023-11-23T13:07:33.376887Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    \"\"\"\n",
    "    In this class, I implement a token object, which store the valuse of a token(str), the list of documents that it is\n",
    "    occurred in, and its index in them.\n",
    "    ...\n",
    "    Attributes:\n",
    "    ----------\n",
    "    word: str\n",
    "        value of a token\n",
    "    docs: list[int] | str\n",
    "        list of documents that it is occurred in or a decoded using gamma code\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word: str):\n",
    "        self.word: str = word\n",
    "        self.docs: list | str = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.word\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### InvertedIndex model\n",
    "Now let's define our main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:33.376552Z",
     "end_time": "2023-11-23T13:07:42.876213Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     UNEXPECTED_EOF_WHILE_READING] EOF occurred in\n",
      "[nltk_data]     violation of protocol (_ssl.c:1006)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     UNEXPECTED_EOF_WHILE_READING] EOF occurred in\n",
      "[nltk_data]     violation of protocol (_ssl.c:1006)>\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from typing import List\n",
    "\n",
    "from src.preprocessing import preprocess_text\n",
    "from src.utils import gamma_code_to_list, list_to_gamma_code\n",
    "\n",
    "\n",
    "class Token:\n",
    "    def __init__(self, word: str):\n",
    "        self.word = word\n",
    "        self.docs = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.word\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.word\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "    \"\"\"\n",
    "    In this class, I implement an information retrieval system which can search a query among documents.\n",
    "    ...\n",
    "    Attributes:\n",
    "    -----------\n",
    "    documents: List\n",
    "        list of documents in format of string.\n",
    "    posting_list: List[Token]\n",
    "        list of Token objects. Tokens store a string and document's indexes\n",
    "    stop_word: set\n",
    "        set of stop words to check when tokenizing\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    Methods defined here:\n",
    "        __init__(self, documents: List):\n",
    "            Constructor will set initial attributes like documents. NOTE that documents should be\n",
    "            list of strings at first.\n",
    "            :parameter\n",
    "            ---------\n",
    "            documents:List\n",
    "                list of strings at first. but then chagnes to list of lists of strings\n",
    "            :return\n",
    "                None\n",
    "\n",
    "        create_posting_list(self):\n",
    "            calling this function, will create posting list of all occurred words cross all documents. in this function,\n",
    "            we use BSBI algorithm, we read batch of documents from disk and create multiple posting_list with that\n",
    "            batch size and save them on disk. In the end, we merge all these batch posting lists. Clearly, in this\n",
    "            implementation we use less RAM because we don't read all documents at once.\n",
    "            :parameter\n",
    "                None\n",
    "            :return\n",
    "                None\n",
    "        get_token_index(self, x):\n",
    "            this function finds index of a word in posting list using binary search algorithm.\n",
    "            :parameter\n",
    "                x:str\n",
    "                    the word you want to find its index\n",
    "            :return\n",
    "                int: index of the word in posting_list\n",
    "        get_token(self, token):\n",
    "                This function will return the token object that contains docs information. if the given token is not in the\n",
    "                posting_list, it returns the spell corrected token.\n",
    "                :param token:\n",
    "                    token you want to fetch it from posting list\n",
    "                :return:\n",
    "                    return the instance of token from posting list\n",
    "        get_word_docs(self, word: str):\n",
    "            this simple function gets a token and will return all indexes of documents that this token is appeared in.\n",
    "            :param word:\n",
    "                a word that you want to search.\n",
    "            :return:\n",
    "                list of indexes of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor will set initial attributes like documents. NOTE that documents should be\n",
    "            list of strings at first.\n",
    "            :parameter\n",
    "            ---------\n",
    "            documents:List\n",
    "                list of strings at first. but then changes to list of lists of strings\n",
    "            :return\n",
    "                None\n",
    "        \"\"\"\n",
    "        self.document_paths: List = []\n",
    "        self.posting_list: List[Token] = []\n",
    "        self.stop_words: set = set(nltk.corpus.stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "    def get_document_paths(self, path=os.getcwd()):\n",
    "        for filename in sorted(os.listdir(path)):\n",
    "            filepath = os.path.join(path, filename)\n",
    "            if filepath.endswith('.txt'):\n",
    "                # Use encoding cp1252 for test cases if an error raised\n",
    "                self.document_paths.append(filepath)\n",
    "        return self.document_paths\n",
    "\n",
    "    def create_posting_list(self, batch_size=1):\n",
    "        \"\"\"\n",
    "        calling this function, will create posting list of all occurred words cross all documents. in this function, we\n",
    "        loop over all documents_path and read numbers of them(with batch size), then inside this loop, we loop over all\n",
    "        the tokens that are in the current document. then we check if the length of posting_list is zero, then we add\n",
    "        this token as first word. else if the length of posting_list is more than 0, we find the correct index of the\n",
    "        token in posting_list alphabetically. then we check if this token, has been already in posting_list, we just add\n",
    "        the current document index in tokens.docs, else, we add this token in the posting_list, then add the current\n",
    "        document index. And when all documents have been indexed and the batch posting lists are saved, we read them from\n",
    "        disk and merge them into main posting list.\n",
    "            :parameter\n",
    "                None\n",
    "            :return\n",
    "                None\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        temp_posting_list = []\n",
    "        for doc_idx in range(len(self.document_paths)):\n",
    "            if doc_idx % batch_size == 0:\n",
    "                temp_posting_list = []\n",
    "            doc_path = self.document_paths[doc_idx]\n",
    "            doc = open(doc_path, encoding='cp1252').read()\n",
    "            doc = preprocess_text(doc)\n",
    "            batch_idx = doc_idx // batch_size\n",
    "            for token in doc:\n",
    "                if temp_posting_list == 0:\n",
    "                    temp_posting_list.append(Token(token))\n",
    "                    temp_posting_list[0].docs.append(doc_idx)\n",
    "                    continue\n",
    "                i = 0\n",
    "                while i < len(temp_posting_list) and token > temp_posting_list[i].word:\n",
    "                    i += 1\n",
    "                if i == len(temp_posting_list):\n",
    "                    temp_posting_list.append(Token(token))\n",
    "                    # self.posting_list[i].post_idx.append(post_idx)\n",
    "                elif token != temp_posting_list[i].word:\n",
    "                    temp_posting_list.insert(i, Token(token))\n",
    "\n",
    "                if doc_idx not in temp_posting_list[i].docs:\n",
    "                    temp_posting_list[i].docs.append(doc_idx)\n",
    "\n",
    "            if (doc_idx + 1) % batch_size == 0:\n",
    "                self.posting_list_to_json(temp_posting_list,\n",
    "                                          path=os.path.join(os.getcwd(), f\"posting_lists/{batch_idx}.json\"))\n",
    "        self.posting_list_to_json(temp_posting_list,\n",
    "                                  path=os.path.join(os.getcwd(), f\"posting_lists/{batch_idx}.json\"))\n",
    "\n",
    "        path = os.path.join(os.getcwd(), 'posting_lists')\n",
    "        batch_posting_lists = []\n",
    "        for filename in sorted(os.listdir(path)):\n",
    "            filepath = os.path.join(path, filename)\n",
    "            if filepath.endswith('.json'):\n",
    "                # Use encoding cp1252 for test cases if an error raised\n",
    "                batch_posting_lists.append(filepath)\n",
    "        curr_merge_posting_list = self.json_to_posting_list(batch_posting_lists[0])\n",
    "        for i in range(len(batch_posting_lists) - 1):\n",
    "            temp_posting_list = self.json_to_posting_list(batch_posting_lists[i + 1])\n",
    "            curr_merge_posting_list = self.merge_two_posting_list(curr_merge_posting_list,\n",
    "                                                                  temp_posting_list)\n",
    "        self.posting_list = curr_merge_posting_list\n",
    "        # for token in self.posting_list:\n",
    "        #     token.docs = list_to_gamma_code(token.docs)\n",
    "\n",
    "    def posting_list_to_json(self, posting_list, path):\n",
    "        json_posting_list = {}\n",
    "        for token in posting_list:\n",
    "            json_posting_list[token.word] = token.docs\n",
    "        with open(path, \"w\") as outfile:\n",
    "            json.dump(json_posting_list, outfile)\n",
    "        return json_posting_list\n",
    "\n",
    "    def json_to_posting_list(self, path):\n",
    "        with open(path, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            posting_list = []\n",
    "            for word, docs in json_object.items():\n",
    "                token = Token(word)\n",
    "                token.docs = docs\n",
    "                posting_list.append(token)\n",
    "            return posting_list\n",
    "\n",
    "    def merge_two_posting_list(self, posting_list_1: List[Token], posting_list_2: List[Token]):\n",
    "        \"\"\"\n",
    "        In this function we merge two posting list. If posting lists contain tokens that their docs attribute is a str,\n",
    "        we should first convert their docs to list of numbers, then merge them and at last, we convert their docs to\n",
    "        gamma code.\n",
    "        :param posting_list_1: First posting list we want to merge\n",
    "        :param posting_list_2: Second posting list we want to merge\n",
    "        :return:\n",
    "            returns a merged posting list of first and second posting list.\n",
    "        \"\"\"\n",
    "        merged_posting_list = posting_list_1\n",
    "        if type(posting_list_1[0].docs) != list:\n",
    "            for token in posting_list_1:\n",
    "                token.docs = gamma_code_to_list(token.docs)\n",
    "        if type(posting_list_2[0].docs) != list:\n",
    "            for token in posting_list_2:\n",
    "                token.docs = gamma_code_to_list(token.docs)\n",
    "        for token in posting_list_2:\n",
    "\n",
    "            low = 0\n",
    "            high = len(merged_posting_list) - 1\n",
    "            idx = 0\n",
    "            while low <= high:\n",
    "                idx = (high + low) // 2\n",
    "                if merged_posting_list[idx].word < token.word:\n",
    "                    low = idx + 1\n",
    "                elif merged_posting_list[idx].word > token.word:\n",
    "                    high = idx - 1\n",
    "                else:\n",
    "                    break\n",
    "            if token.word == merged_posting_list[idx].word:\n",
    "                for doc in token.docs:\n",
    "                    if doc not in merged_posting_list[idx].docs:\n",
    "                        merged_posting_list[idx].docs.append(doc)\n",
    "            else:\n",
    "                merged_posting_list.insert(idx, token)\n",
    "        if type(posting_list_2[0].docs) == list:\n",
    "            for token in posting_list_1:\n",
    "                token.docs = list_to_gamma_code(token.docs)\n",
    "        return merged_posting_list\n",
    "\n",
    "    def get_token_index(self, x, posting_list=None):\n",
    "        \"\"\"\n",
    "        this function find index of a word in posting list using binary search algorithm.\n",
    "            :parameter\n",
    "                x:str\n",
    "                    the word you want to find its index\n",
    "            :return\n",
    "                int: index of the word in posting_list\n",
    "        \"\"\"\n",
    "        if not posting_list:\n",
    "            posting_list = self.posting_list\n",
    "        low = 0\n",
    "        high = len(posting_list) - 1\n",
    "        mid = 0\n",
    "        while low <= high:\n",
    "            mid = (high + low) // 2\n",
    "            if posting_list[mid].word < x:\n",
    "                low = mid + 1\n",
    "            elif posting_list[mid].word > x:\n",
    "                high = mid - 1\n",
    "            else:\n",
    "                return mid\n",
    "        return -1\n",
    "\n",
    "    def get_token(self, token):\n",
    "        \"\"\"\n",
    "        This function will return the token object that contains docs informations. if the given token is not in the\n",
    "        posting_list, it return the spell corrected token.\n",
    "        :param token:\n",
    "            token you want to fetch it from posting list\n",
    "        :return:\n",
    "            return the instance of token from posting list\n",
    "        \"\"\"\n",
    "        p = self.get_token_index(token)\n",
    "        if p == -1:\n",
    "            null_token = Token('token')\n",
    "            null_token.docs = []\n",
    "            return null_token\n",
    "        return self.posting_list[p]\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this class, I implement an information retrieval system which can search a query among documents.\n",
    "    The attributes are:\n",
    "   * documents: List\n",
    "        list of documents in format of string.\n",
    "   * posting_list: List[Token]\n",
    "        list of Token objects. Tokens store a string and document's indexes\n",
    "   * stop_word: set\n",
    "        set of stop words to check when tokenizing\n",
    "\n",
    "### Methods\n",
    "Methods defined here:\n",
    "\n",
    "   * __init__(self, documents: List, case_sensitive=False):\n",
    "        Constructor will set initial attributes like documents and case_sensitive. NOTE that documents should be\n",
    "        list of strings at first.\n",
    "        and parameters are:\n",
    "        * documents:List\n",
    "            list of strings at first. but then chagnes to list of lists of strings\n",
    "        and it returns nothing\n",
    "\n",
    "   * tokenize_document(self, document, use_nltk=False):\n",
    "        in this function, we tokenize a documents. if use_nltk=True, we tokenize using nltk library. if case_sensitive has been set to True, we lower all tokens.\n",
    "        parameters are\n",
    "        * use_nltk: bool, Optional\n",
    "        * use_nltk: bool, Optional\n",
    "        and it returns nothing\n",
    "\n",
    "   * create_posting_list(self):\n",
    "        calling this function, will create posting list of all occurred words cross all documents. in this function,\n",
    "            we use BSBI algorithm, we read batch of a documents from disk and create multiple posting_list with that\n",
    "            batch size and save them on disk. At the end we merge all these batch posting lists. Clearly, in this\n",
    "            implementation we use less RAM, because we don't read all documents at once.\n",
    "\n",
    "   * merge_two_posting_list(self, posting_list_1: List[Token], posting_list_2: List[Token]):\n",
    "            In this function we merge two posting list. If posting lists contain tokens that their docs attribute is a str,\n",
    "            we should first convert their docs to list of numbers, then merge them and at last, we convert their docs to\n",
    "            gamma code.\n",
    "            :param posting_list_1: First posting list we want to merge\n",
    "            :param posting_list_2: Second posting list we want to merge\n",
    "            :return:\n",
    "                returns a merged posting list of first and second posting list.\n",
    "\n",
    "   * get_token_index(self, x):\n",
    "        this function find index of a word in posting list using binary search algorithm.\n",
    "        parameters are\n",
    "        * x:str\n",
    "                the word you want to find its index\n",
    "        and it returns:\n",
    "        * int: index of the word in posting_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Querying\n",
    "At last we complete our IR system by implementing a QueryProcessor class. This class will use the InvertedIndex class we defined before."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "    \"\"\"\n",
    "    In this class, I implement an information retrieval system which can search a query among documents.\n",
    "    ...\n",
    "    Attributes:\n",
    "    -----------\n",
    "    indexing_model: InvertedIndex\n",
    "        an instance of InvertedIndex class which has been created by indexing documents.\n",
    "    Methods\n",
    "    -------\n",
    "    Methods defined here:\n",
    "        get_word_docs(self, word: str):\n",
    "                this simple function gets a token and will return all index of documents that this token is appeared in.\n",
    "                :param word:\n",
    "                    a word that you want to search.\n",
    "                :return:\n",
    "                    list of indexes of documents.\n",
    "        intersect(self, first_word, second_word):\n",
    "                this function get two words, and find documents that both of these word has been occurred.\n",
    "                :parameter\n",
    "                first_word: str\n",
    "                    first word you want to search\n",
    "                second_word: str\n",
    "                    second word you want to search\n",
    "                :return\n",
    "                    list of indexes of documents.\n",
    "            union(self, first_word, second_word):\n",
    "                this function get two words, and find documents that either each of these word has been occurred.\n",
    "                :parameter\n",
    "                first_word: str\n",
    "                    first word you want to search\n",
    "                second_word: str\n",
    "                    second word you want to search\n",
    "                :return\n",
    "                    list of indexes of documents.\n",
    "            not_in(self, word):\n",
    "                this function get one word, and find documents that this word has been not occurred.\n",
    "                :parameter\n",
    "                word: str\n",
    "                    the word you want to search\n",
    "                :return\n",
    "                    list of indexes of documents.\n",
    "            near(self, first_word, second_word, length):\n",
    "                this function get two words, and find documents that either each of these word has been occurred near by at most 3 words on left or right.\n",
    "                :parameter\n",
    "                first_word: str\n",
    "                    first word you want to search\n",
    "                second_word: str\n",
    "                    second word you want to search\n",
    "                :return\n",
    "                    list of indexes of documents.\n",
    "            search(self, query):\n",
    "                this function get a query and recognize what kind of query is; then search the query.\n",
    "                :parameter\n",
    "                    query: str\n",
    "                        the query that user wants to search\n",
    "                :return\n",
    "                    print list of indexes of documents in a pretty way.\n",
    "    \"\"\"\n",
    "    def __init__(self, indexing_model):\n",
    "        self.indexing_model = indexing_model\n",
    "\n",
    "    def get_word_docs(self, word: str):\n",
    "        \"\"\"\n",
    "        This simple function gets a token and will return all index of documents that this token is appeared in. It first\n",
    "        gets the gamma-coded lists, and then convert it to a list of numbers.\n",
    "        :param word:\n",
    "            a word that you want to search.\n",
    "        :return:\n",
    "            list of indexes of documents.\n",
    "        \"\"\"\n",
    "        t = self.indexing_model.get_token(word)\n",
    "        result = set(gamma_code_to_list(t.docs))\n",
    "        return result\n",
    "\n",
    "    def intersect(self, first_word, second_word):\n",
    "        \"\"\"\n",
    "        this function get two words, and find documents that both of these word has been occurred. we first find index of\n",
    "        each word, then in posting list, we find indexes of documents that this word is occurred in. at last, we add the\n",
    "        index of documents in result, if it exists in both token.docs.\n",
    "            :parameter\n",
    "            first_word: str\n",
    "                first word you want to search\n",
    "            second_word: str\n",
    "                second word you want to search\n",
    "            :return\n",
    "                list of indexes of documents.\n",
    "        \"\"\"\n",
    "        t1 = self.get_word_docs(first_word)\n",
    "        t2 = self.get_word_docs(second_word)\n",
    "        if len(t1) > len(t2):\n",
    "            t1, t2 = t2, t1\n",
    "        result = []\n",
    "        for doc1 in t1:\n",
    "            for doc2 in t2:\n",
    "                if doc1 == doc2:\n",
    "                    result.append(doc1)\n",
    "        return result\n",
    "\n",
    "    def union(self, first_word, second_word):\n",
    "        \"\"\"\n",
    "            this function get two words, and find documents that either each of these word has been occurred. we first\n",
    "            find index of each word, then in posting list, we find indexes of documents that each word is occurred in.\n",
    "            at last, we add all found index of documents in result.\n",
    "                :parameter\n",
    "                first_word: str\n",
    "                    first word you want to search\n",
    "                second_word: str\n",
    "                    second word you want to search\n",
    "                :return\n",
    "                    list of indexes of documents.\n",
    "        \"\"\"\n",
    "        t1 = self.get_word_docs(first_word)\n",
    "        t2 = self.get_word_docs(second_word)\n",
    "        if len(t1) > len(t2):\n",
    "            t1, t2 = t2, t1\n",
    "        result = set()\n",
    "        for doc1 in t1:\n",
    "            result.add(doc1)\n",
    "        for doc2 in t2:\n",
    "            result.add(doc2)\n",
    "        return list(result)\n",
    "\n",
    "    def not_in(self, word):\n",
    "        \"\"\"\n",
    "        not_in(self, word):\n",
    "            this function get one word, and find documents that this word has been not occurred. we loop over all docs of\n",
    "            that this token is in and store it in p1_docs. then we return index of documents that are not in hat p1_docs.\n",
    "            :parameter\n",
    "            word: str\n",
    "                the word you want to search\n",
    "            :return\n",
    "                list of indexes of documents.\n",
    "        \"\"\"\n",
    "        t = self.get_word_docs(word)\n",
    "        t_docs = []\n",
    "        for doc in t:\n",
    "            t_docs.append(doc)\n",
    "        result = set()\n",
    "        for idx in range(len(self.indexing_model.document_paths)):\n",
    "            if idx not in t_docs:\n",
    "                result.add(idx)\n",
    "        return list(result)\n",
    "\n",
    "    def search(self, query):\n",
    "        query_parts = query.lower().split()\n",
    "        if 'and' in query_parts:\n",
    "            return self.intersect(query_parts[0], query_parts[2])\n",
    "        elif 'or' in query_parts:\n",
    "            return self.union(query_parts[0], query_parts[2])\n",
    "        elif 'not' in query_parts:\n",
    "            return self.not_in(query_parts[1])\n",
    "        elif 'near' in query:\n",
    "            distance = int(query_parts[1].split('/')[1])\n",
    "            return self.near(query_parts[0], query_parts[2], distance)\n",
    "        else:\n",
    "            return list(self.get_word_docs(query_parts[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:42.884919Z",
     "end_time": "2023-11-23T13:07:42.886790Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Build our system\n",
    "Now let's build our system and call initial functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:42.887226Z",
     "end_time": "2023-11-23T13:07:42.950098Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the Information Retrieval System with the preprocessed documents\n",
    "inverted_index = InvertedIndex()\n",
    "# Get documents' path\n",
    "dataset_path = '../dataset/raw'\n",
    "inverted_index.get_document_paths(path=dataset_path)\n",
    "# Tokenize documents\n",
    "inverted_index.create_posting_list(batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:42.951067Z",
     "end_time": "2023-11-23T13:07:42.953880Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** ir_system documents:\n",
      " ['../dataset/raw/A Festival of Books.txt', '../dataset/raw/A Murder-Suicide.txt', '../dataset/raw/Better To Be Unlucky.txt', '../dataset/raw/Cloning Pets.txt', '../dataset/raw/Crazy Housing Prices.txt', '../dataset/raw/Food Fight Erupted in Prison.txt', '../dataset/raw/Freeway Chase Ends at Newsstand.txt', '../dataset/raw/Gasoline Prices Hit Record High.txt', '../dataset/raw/Happy and Unhappy Renters.txt', '../dataset/raw/Jerry Decided To Buy a Gun.txt', '../dataset/raw/Man Injured at Fast Food Place.txt', '../dataset/raw/Pulling Out Nine Tons of Trash.txt', '../dataset/raw/Rentals at the Oceanside Community.txt', '../dataset/raw/Sara Went Shopping.txt', '../dataset/raw/Trees Are a Threat.txt']\n"
     ]
    }
   ],
   "source": [
    "print('**** ir_system documents:\\n', inverted_index.document_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:42.955120Z",
     "end_time": "2023-11-23T13:07:42.966923Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** ir_system posting list:\n",
      " [1, 10, 1000, 1000x, 100x, 10x, 11, 12, 110000, 120, 120000, 15, 150, 15percent, 1992, 1993, 1995, 1x, 20, 2000, 200, 209, 20000, 222, 214, 24yearold, 230000, 25, 25000, 25000we, 280, 3, 2995, 3037, 30, 350, 38, 300, 4000, 3995, 4000residents, 5, 50, 500, 50000, 500x, 50th, 50x, 510000it, 60, 65yearold, 6000, 7, 70000, 70yearold, 75, 74yearold, 75000, 76, 79yearold, 87, 9, 90, 900, 99, 9000, able, abuse, accidentally, acres, according, actually, administration, afghan, agency, ago, alan, aid, allotments, allow, allen, allowing, almost, along, alone, already, also, altadena, although, always, ambulance, american, ammunition, among, amount, amounts, amputated, angelenos, angeles, anniversary, another, antennas, annual, apartment, anyone, anything, apartments, apparent, appears, apply, appliances, approved, april, area, arcadia, arizona, arrived, around, asked, asking, assisted, attendance, attract, attitudes, audience, auto, authors, autograph, availablethis, avenue, avoid, average, away, avoided, awaybarget, back, baby, bags, back, bad, backbreaking, bags, bag, balls, baldwin, ban, bancity, bang, band, barget, barco, barneys, basic, bartering, batteries, bay, basis, beach, become, behind, believes, berserk, best, beloved, bicycles, better, big, bigger, bikers, binocularsdifferent, bill, birder, biopsy, birding, births, bizarre, black, blanket, block, blind, blocks, blue, bob, book, bookstore, boon, books, boots, born, bottles, bought, bottle, bowling, boxes, bought, braked, boy, brand, brakes, breakfast, brown, bring, brush, bubble, budget, bulbs, bummer, bullet, burger, burn, burned, burst, busjerry, bus, buy, caliber, california, californiaits, called, came, candy, cane, cans, canine, cant, canton, captured, car, careful, carpenter, carriages, cars, carts, carsam, carts, carson, cat, cause, cataract, causing, caused, cell, cells, cents, certificatesone, change, changesara, changed, changing, channels, charged, chasethey, chase, cheap, chef, cheaper, check, childless, children, choose, cigarettes, church, cigars, cities, citys, city, cleared, clearing, clean, clicked, clockwork, clients, clone, clones, cloning, close, clothing, club, coffee, coffers, clubsmuch, coin, collection, collision, color, colors, come, community, comic, company, complain, complaints, complex, completely, complied, complications, concerned, condo, cone, condominium, considerate, consideration, cons, consisting, contestant, continued, conversations, convertible, correctional, correct, correctly, cost, could, counts, couldnt, county, couple, course, court, cream, crazy, created, crew, crews, creek, crossing, crosswalk, cub, cube, cup, cultured, currently, customersthe, cut, customers, cut, cutting, cycles, damage, daily, day, days, daythere, deal, deals, debating, debris, death, decide, decided, decision, delightful, decisionfinally, delivered, delivers, demanding, department, departments, demands, details, desperate, destroyed, diabetes, dictionaries, diabetic, didnt, died, dies, different, directions, dining, dirt, director, discovered, disease, doesnt, dollars, dolls, dogs, dom, dominic, donate, done, donna, donorthe, door, doors, dont, doorslammer, downtown, dozen, dreamily, drinks, drinkspeople, drive, drivers, driving, driver, drivethrough, drizzle, dropped, drove, drought, early, drove, earring, economy, eight, elsewhere, either, elevation, embrace, employee, emphysema, empty, ended, english, enough, ending, environmental, erupted, estimated, estrus, ethnic, even, evening, event, ever, everett, every, everyone, everyones, exact, exchange, except, excitedly, exhibitors, experimenting, extra, extralarge, explain, eye, eyesight, facility, failure, families, fact, fantastic, fault, featured, federal, fee, feet, feel, felix, female, festival, festivals, fiction, filledno, fill, finally, fight, fine, fines, find, fire, first, fiveyearold, five, fixing, flipped, following, food, foods, foot, forward, found, foothill, founders, four, fourdoor, fourslice, francisco, free, freeway, friction, fresh, friendly, front, friends, fruit, full, furniture, future, funds, gallon, future, gallon, gangster, gangsters, garbage, gas, gasoline, game, gave, generating, gear, get, gets, geyser, gift, getting, give, gloves, given, go, goal, goes, golf, going, gonethey, good, got, grabbed, government, governments, grand, great, grocery, groupssave, groupsthe, greater, grow, growing, guarantee, guards, guess, guessed, guessing, gun, guides, guy, guyjerry, gunshots, hadnt, half, halloween, haltturning, hamilton, hamburgers, handed, handle, happen, happened, happy, hauled, havent, hazard, hawaiian, headaches, head, heads, hed, held, heard, help, herman, high, higher, hill, hire, home, hit, holiday, home, hold, homebuyers, homeowner, homes, hope, homeowners, horsetrail, hopes, hot, hospital, hotel, hourly, hour, hours, house, housing, howard, however, huge, hurriedly, hydrant, husband, ice, id, idea, ideas, ill, im, immediately, implanted, inc, income, incomes, inconsiderate, inferno, inevitably, influence, injured, inmates, insurance, interest, invite, irritating, isnt, issued, item, ive, items, jail, japanesehe, ive, jerry, jerrys, job, john, johnson, jobrelated, joke, joy, jumped, kick, kids, killing, kinds, kitten, kneejerry, kittens, knew, landfill, know, landlord, lane, lake, lap, late, last, latest, leading, later, lawn, least, lee, left, leftovers, legal, less, let, letting, libraries, life, light, like, liked, limp, lined, line, lines, lips, liquid, listen, listening, little, lived, living, loan, lives, loans, local, long, look, looking, lookingtim, los, lose, lot, lottery, loud, loved, love, lozano, lowest, lucky, made, luxury, magazine, made, magazines, maintenance, main, major, make, making, man, managed, manager, mans, man, many, market, married, massive, mcrapthe, material, mechanic, medical, meeting, messages, might, mild, middleaged, mile, milk, million, miles, millions, mind, minor, mile, milkplus, minimum, minutes, money, moneya, moneysam, monica, morehowever, month, morning, months, mountain, move, moved, movie, movies, mr, mrs, much, murdersuicide, music, n, must, name, names, nancy, nationwide, nearby, near, necessary, necessarywell, negotiations, neighbor, neighborhood, neighbors, never, nevertheless, newest, new, newspaper, newsstand, next, nextdoor, nice, nicest, night, nightit, nine, nitrogen, north, nobody, noise, nonfat, nonflammable, northville, notify, number, nutritious, numbers, occurred, oceanside, occurs, octane, offer, offers, officer, officers, official, officials, offset, often, oil, ok, officials, old, one, onehour, onebedroom, ones, onto, operationninety, opportunityeveryone, order, others, ought, ordering, outdoor, outdoors, outside, overpaid, overpaying, owners, owns, owner, paid, painmrs, paper, park, paperthe, parked, parking, party, pasadena, part, passing, pay, paying, people, penny, per, people, percent, perfect, personal, permitted, persons, period, phoenix, pianist, phone, piano, pile, pine, picked, pistol, planners, plants, pizza, plates, place, plastic, play, played, player, playgrounda, playing, plowed, plus, pm, plot, police, popular, portable, post, practice, practiced, pretty, president, price, prince, prices, prison, private, prisoners, probably, problem, problems, proceeds, produce, profit, property, proposal, proprietor, pros, proven, provide, pulp, pumping, pursuing, put, puppies, quell, question, questionandanswer, quieter, quite, radios, raging, rain, quietershe, rainstorm, raining, raises, ran, raffle, range, rates, rather, ration, rattle, razors, reading, reads, ready, reason, realtor, recent, reconsider, receives, reduce, reduced, refill, refused, regular, reduction, released, remaining, remarked, remove, removed, rent, rental, rentals, renters, renting, rents, repairs, residents, respond, response, rest, resident, restaurants, restaurant, restored, resulted, resumed, residents, retired, retirees, revolver, return, rick, right, roadside, road, roof, room, ruined, run, safely, sad, said, sale, sam, samantha, san, sandwich, sandwiches, sara, saturday, santa, save, saved, savings, saturday, saxophone, saxophonist, saw, say, saying, says, scaldinghe, scouts, scheduled, seashell, seatbelt, scared, second, secured, security, seekers, seem, seemed, selected, sell, sellers, several, shaking, sent, set, seven, shapes, sherman, shiny, shopping, shave, show, shorter, sicker, side, sickness, sidewalksnoise, sign, silverware, since, six, sixth, sixtyyearold, sizes, slacks, skyrocketing, slam, sleeping, slightly, slowly, slumped, smiled, smith, soap, sneaking, social, sofas, something, sold, sometime, sometimes, sought, southern, southland, space, spanish, specter, sparing, spewed, spill, spilled, spinning, sponsored, sports, stain, spring, stand, standing, stars, state, started, station, stationery, state, stay, steadily, steering, steep, still, stopped, stolen, store, stop, stored, stray, streambed, streets, stretch, street, studio, succeed, successfully, sudden, sue, sued, suggested, summer, supposed, surcharge, sure, surround, surrounded, survive, suffered, suv, sunday, table, take, taken, talk, talked, talking, tank, talks, tax, taxes, teen, teenage, teenager, tell, telling, thats, theater, thelma, themsome, theyd, theres, thick, thing, things, think, thinks, though, three, thought, three, threebedroom, thousands, threehour, threw, tim, time, timethe, times, tired, tires, toaster, today, todays, together, toilet, told, tons, took, toothe, toppled, toss, totally, toward, town, toyola, tower, trade, tough, trash, tree, trees, traffic, tried, triplescoop, try, trucks, trying, tuner, tuneups, turned, tv, twenty, twice, twin, two, twobedroom, twomile, underbrush, unemployed, unfortunately, unhappy, uninjured, uneventful, unlucky, unpredictable, upset, use, us, used, value, variable, variables, vehicles, vehiclewhen, vendor, venice, veterinarian, vicky, victoria, video, videos, visiting, visitors, vittorio, volunteers, vittorios, wait, waiting, waited, walked, walks, warden, want, wash, washes, wasnt, watch, watches, watching, way, water, weaned, wearing, website, week, weekend, weekly, weeks, well, went, whales, west, whatever, wheel, wheels, whether, whiskey, white, whole, whose, widow, widowed, wife, win, windows, winnings, woke, woman, wont, worked, work, world, worms, worth, would, wrong, years, year, yelling, yellow, yet, yield, young, years]\n"
     ]
    }
   ],
   "source": [
    "print('**** ir_system posting list:\\n', inverted_index.posting_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Posting list with their index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:42.966013Z",
     "end_time": "2023-11-23T13:07:42.968978Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: doc_list: 111011\n",
      "10: doc_list: 1011100011011100110010\n",
      "1000: doc_list: 1100111011111001\n",
      "1000x: doc_list: 1100\n",
      "100x: doc_list: 1100\n",
      "10x: doc_list: 1100\n",
      "11: doc_list: 11110110\n",
      "12: doc_list: 111011\n",
      "110000: doc_list: 1100\n",
      "120: doc_list: 1100\n",
      "120000: doc_list: 1100\n",
      "15: doc_list: 11110100\n",
      "150: doc_list: 1011110010\n",
      "15percent: doc_list: 11110100\n",
      "1992: doc_list: 11110101\n",
      "1993: doc_list: 111000\n",
      "1995: doc_list: 11110101\n",
      "1x: doc_list: 1100\n",
      "20: doc_list: 10111010\n",
      "2000: doc_list: 11110100\n",
      "200: doc_list: 11110100\n",
      "209: doc_list: 111011\n",
      "20000: doc_list: 111000\n",
      "222: doc_list: 111011\n",
      "214: doc_list: 111011\n",
      "24yearold: doc_list: 111010\n",
      "230000: doc_list: 111000\n",
      "25: doc_list: 1100\n",
      "25000: doc_list: 1101\n",
      "25000we: doc_list: 1101\n",
      "280: doc_list: 10\n",
      "3: doc_list: 11110110\n",
      "2995: doc_list: 11110101\n",
      "3037: doc_list: 11110101\n",
      "30: doc_list: 1110111100111000\n",
      "350: doc_list: 11110101\n",
      "38: doc_list: 11110001\n",
      "300: doc_list: 11110001\n",
      "4000: doc_list: 11110110\n",
      "3995: doc_list: 11110101\n",
      "4000residents: doc_list: 11110110\n",
      "5: doc_list: 1110001101\n",
      "50: doc_list: 10110011110010\n",
      "500: doc_list: 11110011\n",
      "50000: doc_list: 1101\n",
      "500x: doc_list: 1100\n",
      "50th: doc_list: 10\n",
      "50x: doc_list: 1100\n",
      "510000it: doc_list: 111000\n",
      "60: doc_list: 1101\n",
      "65yearold: doc_list: 11110110\n",
      "6000: doc_list: 11110110\n",
      "7: doc_list: 10\n",
      "70000: doc_list: 10111000\n",
      "70yearold: doc_list: 10\n",
      "75: doc_list: 111001\n",
      "74yearold: doc_list: 10\n",
      "75000: doc_list: 10\n",
      "76: doc_list: 1100\n",
      "79yearold: doc_list: 11110010\n",
      "87: doc_list: 111011\n",
      "9: doc_list: 11110101\n",
      "90: doc_list: 10\n",
      "900: doc_list: 11110010\n",
      "99: doc_list: 111011\n",
      "9000: doc_list: 1100\n",
      "able: doc_list: 11110010\n",
      "abuse: doc_list: 11110100\n",
      "accidentally: doc_list: 1111000110\n",
      "acres: doc_list: 11110110\n",
      "according: doc_list: 10\n",
      "actually: doc_list: 1101\n",
      "administration: doc_list: 111001\n",
      "afghan: doc_list: 1100\n",
      "agency: doc_list: 10\n",
      "ago: doc_list: 10101110001100111000\n",
      "alan: doc_list: 11110011\n",
      "aid: doc_list: 11110010\n",
      "allotments: doc_list: 111001\n",
      "allow: doc_list: 11110100\n",
      "allen: doc_list: 10\n",
      "allowing: doc_list: 11110100\n",
      "almost: doc_list: 1011101011100010\n",
      "along: doc_list: 11110011\n",
      "alone: doc_list: 10\n",
      "already: doc_list: 1010110011110000\n",
      "also: doc_list: 11001110001100101101\n",
      "altadena: doc_list: 10\n",
      "although: doc_list: 1011110010\n",
      "always: doc_list: 101101111000\n",
      "ambulance: doc_list: 111010\n",
      "american: doc_list: 10\n",
      "ammunition: doc_list: 11110001\n",
      "among: doc_list: 10\n",
      "amount: doc_list: 1101\n",
      "amounts: doc_list: 11110100\n",
      "amputated: doc_list: 10\n",
      "angelenos: doc_list: 10\n",
      "angeles: doc_list: 10111010\n",
      "anniversary: doc_list: 10\n",
      "another: doc_list: 11110000111000\n",
      "antennas: doc_list: 11110011\n",
      "annual: doc_list: 1011110000\n",
      "apartment: doc_list: 11110001\n",
      "anyone: doc_list: 111000\n",
      "anything: doc_list: 11110011\n",
      "apartments: doc_list: 11110000\n",
      "apparent: doc_list: 10\n",
      "appears: doc_list: 1100\n",
      "apply: doc_list: 11110110\n",
      "appliances: doc_list: 10\n",
      "approved: doc_list: 111001\n",
      "april: doc_list: 10\n",
      "area: doc_list: 11110110\n",
      "arcadia: doc_list: 111011\n",
      "arizona: doc_list: 1101\n",
      "arrived: doc_list: 11110101\n",
      "around: doc_list: 1110001101\n",
      "asked: doc_list: 111011111011\n",
      "asking: doc_list: 111000\n",
      "assisted: doc_list: 11110011\n",
      "attendance: doc_list: 10\n",
      "attract: doc_list: 111010\n",
      "attitudes: doc_list: 11110000\n",
      "audience: doc_list: 10\n",
      "auto: doc_list: 11110011\n",
      "authors: doc_list: 10\n",
      "autograph: doc_list: 10\n",
      "availablethis: doc_list: 10\n",
      "avenue: doc_list: 1100111000\n",
      "avoid: doc_list: 111010\n",
      "average: doc_list: 111011\n",
      "away: doc_list: 111011\n",
      "avoided: doc_list: 10\n",
      "awaybarget: doc_list: 11110101\n",
      "back: doc_list: 11110011110010\n",
      "baby: doc_list: 11110011\n",
      "bags: doc_list: 111001\n",
      "back: doc_list: 11100110\n",
      "bad: doc_list: 11110110\n",
      "backbreaking: doc_list: 11110011\n",
      "bags: doc_list: 11110011\n",
      "bag: doc_list: 11110011\n",
      "balls: doc_list: 11110011\n",
      "baldwin: doc_list: 11110001\n",
      "ban: doc_list: 11110100\n",
      "bancity: doc_list: 11110100\n",
      "bang: doc_list: 111010\n",
      "band: doc_list: 11110000\n",
      "barget: doc_list: 11110101\n",
      "barco: doc_list: 111011\n",
      "barneys: doc_list: 11110001\n",
      "basic: doc_list: 111001\n",
      "bartering: doc_list: 111001\n",
      "batteries: doc_list: 11110011\n",
      "bay: doc_list: 11110011\n",
      "basis: doc_list: 11110100\n",
      "beach: doc_list: 11100011110000\n",
      "become: doc_list: 10\n",
      "behind: doc_list: 111010\n",
      "believes: doc_list: 1101\n",
      "berserk: doc_list: 111001\n",
      "best: doc_list: 11110000\n",
      "beloved: doc_list: 1101\n",
      "bicycles: doc_list: 11110011\n",
      "better: doc_list: 1100111010\n",
      "big: doc_list: 110011011110101101\n",
      "bigger: doc_list: 1100\n",
      "bikers: doc_list: 11110110\n",
      "binocularsdifferent: doc_list: 11110000\n",
      "bill: doc_list: 1100\n",
      "birder: doc_list: 11110000\n",
      "biopsy: doc_list: 1101\n",
      "birding: doc_list: 11110000\n",
      "births: doc_list: 1101\n",
      "bizarre: doc_list: 1101\n",
      "black: doc_list: 11110001\n",
      "blanket: doc_list: 1100\n",
      "block: doc_list: 111011\n",
      "blind: doc_list: 10\n",
      "blocks: doc_list: 111010\n",
      "blue: doc_list: 11110101\n",
      "bob: doc_list: 11110101\n",
      "book: doc_list: 10\n",
      "bookstore: doc_list: 111010\n",
      "boon: doc_list: 11110100\n",
      "books: doc_list: 10111010\n",
      "boots: doc_list: 11110011\n",
      "born: doc_list: 1101\n",
      "bottles: doc_list: 11110011\n",
      "bought: doc_list: 11110101\n",
      "bottle: doc_list: 111010\n",
      "bowling: doc_list: 11110011\n",
      "boxes: doc_list: 11110001\n",
      "bought: doc_list: 11100011011100\n",
      "braked: doc_list: 111010\n",
      "boy: doc_list: 110011110001\n",
      "brand: doc_list: 11110010\n",
      "brakes: doc_list: 111010\n",
      "breakfast: doc_list: 11110101\n",
      "brown: doc_list: 11110100\n",
      "bring: doc_list: 111000111000111000\n",
      "brush: doc_list: 11110110\n",
      "bubble: doc_list: 111000\n",
      "budget: doc_list: 11110100\n",
      "bulbs: doc_list: 10\n",
      "bummer: doc_list: 111011\n",
      "bullet: doc_list: 11110001\n",
      "burger: doc_list: 11110010\n",
      "burn: doc_list: 11110010\n",
      "burned: doc_list: 11110110\n",
      "burst: doc_list: 111000\n",
      "busjerry: doc_list: 11110001\n",
      "bus: doc_list: 111010\n",
      "buy: doc_list: 11100011100111001100\n",
      "caliber: doc_list: 11110001\n",
      "california: doc_list: 10111011\n",
      "californiaits: doc_list: 111000\n",
      "called: doc_list: 101110011100\n",
      "came: doc_list: 1010101110101010\n",
      "candy: doc_list: 10111000\n",
      "cane: doc_list: 11110001\n",
      "cans: doc_list: 11110011\n",
      "canine: doc_list: 1101\n",
      "cant: doc_list: 1110001101\n",
      "canton: doc_list: 11110110\n",
      "captured: doc_list: 111001\n",
      "car: doc_list: 101111000011001100\n",
      "careful: doc_list: 11110100\n",
      "carpenter: doc_list: 10\n",
      "carriages: doc_list: 11110011\n",
      "cars: doc_list: 110011100010111001\n",
      "carts: doc_list: 111010\n",
      "carsam: doc_list: 1100\n",
      "carts: doc_list: 11110011\n",
      "carson: doc_list: 11110011\n",
      "cat: doc_list: 1101\n",
      "cause: doc_list: 11110100\n",
      "cataract: doc_list: 10\n",
      "causing: doc_list: 111010\n",
      "caused: doc_list: 111010\n",
      "cell: doc_list: 110010\n",
      "cells: doc_list: 1101\n",
      "cents: doc_list: 111011111010\n",
      "certificatesone: doc_list: 11110010\n",
      "change: doc_list: 1100\n",
      "changesara: doc_list: 11110101\n",
      "changed: doc_list: 11110000\n",
      "changing: doc_list: 10\n",
      "channels: doc_list: 1100\n",
      "charged: doc_list: 111010111010\n",
      "chasethey: doc_list: 111010\n",
      "chase: doc_list: 111010\n",
      "cheap: doc_list: 111011\n",
      "chef: doc_list: 11110000\n",
      "cheaper: doc_list: 111011\n",
      "check: doc_list: 11110101\n",
      "childless: doc_list: 10\n",
      "children: doc_list: 11110101\n",
      "choose: doc_list: 1100\n",
      "cigarettes: doc_list: 111001\n",
      "church: doc_list: 1100\n",
      "cigars: doc_list: 111001\n",
      "cities: doc_list: 10\n",
      "citys: doc_list: 11110100\n",
      "city: doc_list: 1011110011\n",
      "cleared: doc_list: 11110110\n",
      "clearing: doc_list: 11110110\n",
      "clean: doc_list: 11110011\n",
      "clicked: doc_list: 1100\n",
      "clockwork: doc_list: 11110000\n",
      "clients: doc_list: 1101\n",
      "clone: doc_list: 1101\n",
      "clones: doc_list: 1101\n",
      "cloning: doc_list: 1101\n",
      "close: doc_list: 10\n",
      "clothing: doc_list: 11110011\n",
      "club: doc_list: 11110001\n",
      "coffee: doc_list: 11110010\n",
      "coffers: doc_list: 11110100\n",
      "clubsmuch: doc_list: 11110011\n",
      "coin: doc_list: 1100\n",
      "collection: doc_list: 11110001\n",
      "collision: doc_list: 111010\n",
      "color: doc_list: 1111000010\n",
      "colors: doc_list: 11110011\n",
      "come: doc_list: 101101110011001101\n",
      "community: doc_list: 1111001110\n",
      "comic: doc_list: 10\n",
      "company: doc_list: 110010\n",
      "complain: doc_list: 11110000\n",
      "complaints: doc_list: 11110100\n",
      "complex: doc_list: 111001\n",
      "completely: doc_list: 10\n",
      "complied: doc_list: 111001\n",
      "complications: doc_list: 10\n",
      "concerned: doc_list: 11110011\n",
      "condo: doc_list: 111000\n",
      "cone: doc_list: 11110011\n",
      "condominium: doc_list: 111000\n",
      "considerate: doc_list: 11110100\n",
      "consideration: doc_list: 11110100\n",
      "cons: doc_list: 11110100\n",
      "consisting: doc_list: 11110011\n",
      "contestant: doc_list: 1100\n",
      "continued: doc_list: 11110100\n",
      "conversations: doc_list: 10\n",
      "convertible: doc_list: 11110010\n",
      "correctional: doc_list: 111001\n",
      "correct: doc_list: 1100\n",
      "correctly: doc_list: 1100\n",
      "cost: doc_list: 110111110011\n",
      "could: doc_list: 1011110000\n",
      "counts: doc_list: 111011\n",
      "couldnt: doc_list: 11110000\n",
      "county: doc_list: 111010\n",
      "couple: doc_list: 101011110000\n",
      "course: doc_list: 111011111000\n",
      "court: doc_list: 10\n",
      "cream: doc_list: 11110011\n",
      "crazy: doc_list: 111000\n",
      "created: doc_list: 11110100\n",
      "crew: doc_list: 11110011\n",
      "crews: doc_list: 11110100\n",
      "creek: doc_list: 11110011\n",
      "crossing: doc_list: 1100\n",
      "crosswalk: doc_list: 111010\n",
      "cub: doc_list: 11110011\n",
      "cube: doc_list: 1100\n",
      "cup: doc_list: 11110010\n",
      "cultured: doc_list: 1101\n",
      "currently: doc_list: 1101\n",
      "customersthe: doc_list: 111011\n",
      "cut: doc_list: 111001\n",
      "customers: doc_list: 111010\n",
      "cut: doc_list: 11110110\n",
      "cutting: doc_list: 11110110\n",
      "cycles: doc_list: 111000\n",
      "damage: doc_list: 111010\n",
      "daily: doc_list: 111001\n",
      "day: doc_list: 1110001110001101\n",
      "days: doc_list: 1101\n",
      "daythere: doc_list: 11110000\n",
      "deal: doc_list: 111001\n",
      "deals: doc_list: 10\n",
      "debating: doc_list: 11110100\n",
      "debris: doc_list: 11110011\n",
      "death: doc_list: 10\n",
      "decide: doc_list: 111000\n",
      "decided: doc_list: 11110001\n",
      "decision: doc_list: 111000\n",
      "delightful: doc_list: 11110000\n",
      "decisionfinally: doc_list: 1100\n",
      "delivered: doc_list: 10\n",
      "delivers: doc_list: 1101\n",
      "demanding: doc_list: 111001\n",
      "department: doc_list: 11110101\n",
      "departments: doc_list: 11110011\n",
      "demands: doc_list: 111001\n",
      "details: doc_list: 11110001\n",
      "desperate: doc_list: 111000\n",
      "destroyed: doc_list: 11110110\n",
      "diabetes: doc_list: 10\n",
      "dictionaries: doc_list: 111001\n",
      "diabetic: doc_list: 10\n",
      "didnt: doc_list: 10111010\n",
      "died: doc_list: 10\n",
      "dies: doc_list: 1101\n",
      "different: doc_list: 1110101100\n",
      "directions: doc_list: 111010\n",
      "dining: doc_list: 111001\n",
      "dirt: doc_list: 11110110\n",
      "director: doc_list: 1110001110011100\n",
      "discovered: doc_list: 111001\n",
      "disease: doc_list: 10\n",
      "doesnt: doc_list: 11001100\n",
      "dollars: doc_list: 11110011\n",
      "dolls: doc_list: 11110011\n",
      "dogs: doc_list: 1101\n",
      "dom: doc_list: 10\n",
      "dominic: doc_list: 10\n",
      "donate: doc_list: 11110011\n",
      "done: doc_list: 11110011\n",
      "donna: doc_list: 111000\n",
      "donorthe: doc_list: 1101\n",
      "door: doc_list: 111001\n",
      "doors: doc_list: 111001\n",
      "dont: doc_list: 1101101101111011\n",
      "doorslammer: doc_list: 11110000\n",
      "downtown: doc_list: 111010\n",
      "dozen: doc_list: 1101\n",
      "dreamily: doc_list: 1100\n",
      "drinks: doc_list: 10\n",
      "drinkspeople: doc_list: 10\n",
      "drive: doc_list: 111011\n",
      "drivers: doc_list: 111010\n",
      "driving: doc_list: 111010\n",
      "driver: doc_list: 111010\n",
      "drivethrough: doc_list: 11110010\n",
      "drizzle: doc_list: 11110011\n",
      "dropped: doc_list: 11110001\n",
      "drove: doc_list: 111100101101\n",
      "drought: doc_list: 11110110\n",
      "early: doc_list: 111010\n",
      "drove: doc_list: 10\n",
      "earring: doc_list: 11110011\n",
      "economy: doc_list: 111011\n",
      "eight: doc_list: 101100111000111000\n",
      "elsewhere: doc_list: 111011\n",
      "either: doc_list: 10\n",
      "elevation: doc_list: 11110110\n",
      "embrace: doc_list: 10\n",
      "employee: doc_list: 11110010\n",
      "emphysema: doc_list: 10\n",
      "empty: doc_list: 111010\n",
      "ended: doc_list: 111010\n",
      "english: doc_list: 11110001\n",
      "enough: doc_list: 111010\n",
      "ending: doc_list: 10\n",
      "environmental: doc_list: 11110011\n",
      "erupted: doc_list: 111001\n",
      "estimated: doc_list: 10\n",
      "estrus: doc_list: 1101\n",
      "ethnic: doc_list: 10\n",
      "even: doc_list: 10101100101101110110\n",
      "evening: doc_list: 11110010\n",
      "event: doc_list: 11110011\n",
      "ever: doc_list: 1100\n",
      "everett: doc_list: 111011\n",
      "every: doc_list: 101011101010110010\n",
      "everyone: doc_list: 101110001110101100\n",
      "everyones: doc_list: 111000\n",
      "exact: doc_list: 111000\n",
      "exchange: doc_list: 111001\n",
      "except: doc_list: 10111010\n",
      "excitedly: doc_list: 1100\n",
      "exhibitors: doc_list: 10\n",
      "experimenting: doc_list: 1101\n",
      "extra: doc_list: 111101001100\n",
      "extralarge: doc_list: 11110010\n",
      "explain: doc_list: 10\n",
      "eye: doc_list: 10\n",
      "eyesight: doc_list: 10\n",
      "facility: doc_list: 1101\n",
      "failure: doc_list: 111010\n",
      "families: doc_list: 11110100\n",
      "fact: doc_list: 10\n",
      "fantastic: doc_list: 10\n",
      "fault: doc_list: 11110010\n",
      "featured: doc_list: 10\n",
      "federal: doc_list: 110011110100\n",
      "fee: doc_list: 1011110011\n",
      "feet: doc_list: 11110110\n",
      "feel: doc_list: 111000\n",
      "felix: doc_list: 1101\n",
      "female: doc_list: 1101111011\n",
      "festival: doc_list: 10\n",
      "festivals: doc_list: 10\n",
      "fiction: doc_list: 11110001\n",
      "filledno: doc_list: 11110011\n",
      "fill: doc_list: 111011\n",
      "finally: doc_list: 11110001\n",
      "fight: doc_list: 111001\n",
      "fine: doc_list: 11110100\n",
      "fines: doc_list: 11110100\n",
      "find: doc_list: 111010\n",
      "fire: doc_list: 1110101110011101\n",
      "first: doc_list: 11001010111000111010\n",
      "fiveyearold: doc_list: 11110011\n",
      "five: doc_list: 11100011100011100110\n",
      "fixing: doc_list: 10\n",
      "flipped: doc_list: 1100\n",
      "following: doc_list: 1011110001\n",
      "food: doc_list: 1010111000\n",
      "foods: doc_list: 10\n",
      "foot: doc_list: 10\n",
      "forward: doc_list: 11110110\n",
      "found: doc_list: 11110011\n",
      "foothill: doc_list: 11110101\n",
      "founders: doc_list: 10\n",
      "four: doc_list: 111100001011011100\n",
      "fourdoor: doc_list: 11110101\n",
      "fourslice: doc_list: 11110101\n",
      "francisco: doc_list: 10\n",
      "free: doc_list: 1011110001\n",
      "freeway: doc_list: 111010\n",
      "friction: doc_list: 11110100\n",
      "fresh: doc_list: 101100111011\n",
      "friendly: doc_list: 10\n",
      "front: doc_list: 111010\n",
      "friends: doc_list: 101101111000\n",
      "fruit: doc_list: 10\n",
      "full: doc_list: 10111001111001\n",
      "furniture: doc_list: 11110011\n",
      "future: doc_list: 11110110\n",
      "funds: doc_list: 11110110\n",
      "gallon: doc_list: 111011\n",
      "future: doc_list: 111000\n",
      "gallon: doc_list: 11110101\n",
      "gangster: doc_list: 11110001\n",
      "gangsters: doc_list: 11110001\n",
      "garbage: doc_list: 11110011\n",
      "gas: doc_list: 111011\n",
      "gasoline: doc_list: 111011\n",
      "game: doc_list: 1100\n",
      "gave: doc_list: 11110010\n",
      "generating: doc_list: 11110100\n",
      "gear: doc_list: 11110011\n",
      "get: doc_list: 1010110111011011011100\n",
      "gets: doc_list: 11110100\n",
      "geyser: doc_list: 111010\n",
      "gift: doc_list: 11110010\n",
      "getting: doc_list: 1100\n",
      "give: doc_list: 111000\n",
      "gloves: doc_list: 11110011\n",
      "given: doc_list: 1011110001\n",
      "go: doc_list: 1011011101101010\n",
      "goal: doc_list: 1101\n",
      "goes: doc_list: 10110110111000\n",
      "golf: doc_list: 11110011\n",
      "going: doc_list: 110011110100\n",
      "gonethey: doc_list: 10\n",
      "good: doc_list: 1011110101\n",
      "got: doc_list: 101011100011101110\n",
      "grabbed: doc_list: 111001\n",
      "government: doc_list: 111000\n",
      "governments: doc_list: 11110110\n",
      "grand: doc_list: 111010\n",
      "great: doc_list: 10111000111010\n",
      "grocery: doc_list: 111011\n",
      "groupssave: doc_list: 11110011\n",
      "groupsthe: doc_list: 11110011\n",
      "greater: doc_list: 1100\n",
      "grow: doc_list: 1101\n",
      "growing: doc_list: 1101\n",
      "guarantee: doc_list: 1100\n",
      "guards: doc_list: 111001\n",
      "guess: doc_list: 1100\n",
      "guessed: doc_list: 1100\n",
      "guessing: doc_list: 1100\n",
      "gun: doc_list: 11110001\n",
      "guides: doc_list: 10\n",
      "guy: doc_list: 11110000\n",
      "guyjerry: doc_list: 11110001\n",
      "gunshots: doc_list: 10\n",
      "hadnt: doc_list: 10\n",
      "half: doc_list: 101011001100111001\n",
      "halloween: doc_list: 10\n",
      "haltturning: doc_list: 111010\n",
      "hamilton: doc_list: 11110110\n",
      "hamburgers: doc_list: 10\n",
      "handed: doc_list: 10\n",
      "handle: doc_list: 1101\n",
      "happen: doc_list: 111010\n",
      "happened: doc_list: 10\n",
      "happy: doc_list: 11110000\n",
      "hauled: doc_list: 111001111010\n",
      "havent: doc_list: 10\n",
      "hazard: doc_list: 11110110\n",
      "hawaiian: doc_list: 10\n",
      "headaches: doc_list: 11110100\n",
      "head: doc_list: 11110000\n",
      "heads: doc_list: 1100\n",
      "hed: doc_list: 11110000\n",
      "held: doc_list: 111001\n",
      "heard: doc_list: 10\n",
      "help: doc_list: 1011110101\n",
      "herman: doc_list: 11110010\n",
      "high: doc_list: 111000111100001100\n",
      "higher: doc_list: 1110001101\n",
      "hill: doc_list: 1100\n",
      "hire: doc_list: 11110010\n",
      "home: doc_list: 1111000010\n",
      "hit: doc_list: 1100\n",
      "holiday: doc_list: 11110101\n",
      "home: doc_list: 11110101\n",
      "hold: doc_list: 11110100\n",
      "homebuyers: doc_list: 111000\n",
      "homeowner: doc_list: 11110100\n",
      "homes: doc_list: 111101001100\n",
      "hope: doc_list: 11110011\n",
      "homeowners: doc_list: 11110100\n",
      "horsetrail: doc_list: 111011\n",
      "hopes: doc_list: 111000\n",
      "hot: doc_list: 111100101101\n",
      "hospital: doc_list: 1100111000\n",
      "hotel: doc_list: 111010\n",
      "hourly: doc_list: 11110100\n",
      "hour: doc_list: 10\n",
      "hours: doc_list: 11001110101101\n",
      "house: doc_list: 10110111110000\n",
      "housing: doc_list: 111000\n",
      "howard: doc_list: 11110000\n",
      "however: doc_list: 111010\n",
      "huge: doc_list: 11110100\n",
      "hurriedly: doc_list: 111010\n",
      "hydrant: doc_list: 111010\n",
      "husband: doc_list: 10\n",
      "ice: doc_list: 1011110010\n",
      "id: doc_list: 11110000\n",
      "idea: doc_list: 10111000\n",
      "ideas: doc_list: 11110110\n",
      "ill: doc_list: 111000\n",
      "im: doc_list: 1011110101\n",
      "immediately: doc_list: 10\n",
      "implanted: doc_list: 1101\n",
      "inc: doc_list: 1101\n",
      "income: doc_list: 11110100\n",
      "incomes: doc_list: 11110100\n",
      "inconsiderate: doc_list: 11110000111000\n",
      "inferno: doc_list: 11110110\n",
      "inevitably: doc_list: 111000\n",
      "influence: doc_list: 111010\n",
      "injured: doc_list: 11110010\n",
      "inmates: doc_list: 111001\n",
      "insurance: doc_list: 1100\n",
      "interest: doc_list: 101101\n",
      "invite: doc_list: 11110001\n",
      "irritating: doc_list: 11110000\n",
      "isnt: doc_list: 11110100\n",
      "issued: doc_list: 11110011\n",
      "item: doc_list: 111001\n",
      "ive: doc_list: 11110110\n",
      "items: doc_list: 111001\n",
      "jail: doc_list: 111010\n",
      "japanesehe: doc_list: 11110001\n",
      "ive: doc_list: 10111011\n",
      "jerry: doc_list: 11110001\n",
      "jerrys: doc_list: 11110001\n",
      "job: doc_list: 11110011\n",
      "john: doc_list: 111100101101\n",
      "johnson: doc_list: 11110010\n",
      "jobrelated: doc_list: 11110000\n",
      "joke: doc_list: 10\n",
      "joy: doc_list: 1100\n",
      "jumped: doc_list: 1100\n",
      "kick: doc_list: 111000\n",
      "kids: doc_list: 1011110100\n",
      "killing: doc_list: 111011\n",
      "kinds: doc_list: 1010\n",
      "kitten: doc_list: 1101\n",
      "kneejerry: doc_list: 11110001\n",
      "kittens: doc_list: 1101\n",
      "knew: doc_list: 1011110000\n",
      "landfill: doc_list: 11110011\n",
      "know: doc_list: 111000111100001100\n",
      "landlord: doc_list: 11110000\n",
      "lane: doc_list: 11110010\n",
      "lake: doc_list: 1100\n",
      "lap: doc_list: 11110010\n",
      "late: doc_list: 11110100\n",
      "last: doc_list: 101101\n",
      "latest: doc_list: 11110000\n",
      "leading: doc_list: 111010\n",
      "later: doc_list: 110110111010\n",
      "lawn: doc_list: 11110100\n",
      "least: doc_list: 111000\n",
      "lee: doc_list: 1101\n",
      "left: doc_list: 1100111010\n",
      "leftovers: doc_list: 11110000\n",
      "legal: doc_list: 111001\n",
      "less: doc_list: 10\n",
      "let: doc_list: 11110010\n",
      "letting: doc_list: 11110010\n",
      "libraries: doc_list: 10\n",
      "life: doc_list: 10101110101101\n",
      "light: doc_list: 1011110010\n",
      "like: doc_list: 10110110110110\n",
      "liked: doc_list: 11110001\n",
      "limp: doc_list: 11110001\n",
      "lined: doc_list: 11110011\n",
      "line: doc_list: 111011\n",
      "lines: doc_list: 111011\n",
      "lips: doc_list: 111000\n",
      "liquid: doc_list: 1101\n",
      "listen: doc_list: 11110000111000\n",
      "listening: doc_list: 10\n",
      "little: doc_list: 1100110011110010\n",
      "lived: doc_list: 1011110000111000\n",
      "living: doc_list: 11110110\n",
      "loan: doc_list: 11110110\n",
      "lives: doc_list: 11011100\n",
      "loans: doc_list: 11110110\n",
      "local: doc_list: 101101\n",
      "long: doc_list: 1110001101\n",
      "look: doc_list: 11110110\n",
      "looking: doc_list: 11100011110010\n",
      "lookingtim: doc_list: 111000\n",
      "los: doc_list: 10111010\n",
      "lose: doc_list: 101101\n",
      "lot: doc_list: 10111010111001\n",
      "lottery: doc_list: 1100\n",
      "loud: doc_list: 11110100\n",
      "loved: doc_list: 11110001\n",
      "love: doc_list: 10\n",
      "lozano: doc_list: 11110100\n",
      "lowest: doc_list: 111011\n",
      "lucky: doc_list: 1110101100111010\n",
      "made: doc_list: 11110101\n",
      "luxury: doc_list: 111001\n",
      "magazine: doc_list: 111010\n",
      "made: doc_list: 111000\n",
      "magazines: doc_list: 10111001\n",
      "maintenance: doc_list: 11110100\n",
      "main: doc_list: 111001\n",
      "major: doc_list: 11110110\n",
      "make: doc_list: 11110110\n",
      "making: doc_list: 111011111001\n",
      "man: doc_list: 11110010\n",
      "managed: doc_list: 111010\n",
      "manager: doc_list: 1110111100101100\n",
      "mans: doc_list: 11110010\n",
      "man: doc_list: 101110011010\n",
      "many: doc_list: 1011110000\n",
      "market: doc_list: 111000\n",
      "married: doc_list: 1011110100\n",
      "massive: doc_list: 11110110\n",
      "mcrapthe: doc_list: 11110010\n",
      "material: doc_list: 10\n",
      "mechanic: doc_list: 11110000\n",
      "medical: doc_list: 11110010\n",
      "meeting: doc_list: 11110100\n",
      "messages: doc_list: 111001\n",
      "might: doc_list: 111100101010\n",
      "mild: doc_list: 11110010\n",
      "middleaged: doc_list: 11110000\n",
      "mile: doc_list: 11110101\n",
      "milk: doc_list: 11110101\n",
      "million: doc_list: 111100111101\n",
      "miles: doc_list: 111011\n",
      "millions: doc_list: 11110000\n",
      "mind: doc_list: 111011\n",
      "minor: doc_list: 111010\n",
      "mile: doc_list: 11110001\n",
      "milkplus: doc_list: 11110101\n",
      "minimum: doc_list: 11110110\n",
      "minutes: doc_list: 1110111100\n",
      "money: doc_list: 1111000111011100\n",
      "moneya: doc_list: 111011\n",
      "moneysam: doc_list: 1100\n",
      "monica: doc_list: 111000\n",
      "morehowever: doc_list: 11110100\n",
      "month: doc_list: 1010\n",
      "morning: doc_list: 11110001\n",
      "months: doc_list: 111011\n",
      "mountain: doc_list: 11110110\n",
      "move: doc_list: 111000\n",
      "moved: doc_list: 11110000\n",
      "movie: doc_list: 1011110001\n",
      "movies: doc_list: 1011110001\n",
      "mr: doc_list: 10\n",
      "mrs: doc_list: 10\n",
      "much: doc_list: 1100111001\n",
      "murdersuicide: doc_list: 10\n",
      "music: doc_list: 11110000111000\n",
      "n: doc_list: 11110101\n",
      "must: doc_list: 1100101011110010\n",
      "name: doc_list: 11110001\n",
      "names: doc_list: 11110100\n",
      "nancy: doc_list: 11110101\n",
      "nationwide: doc_list: 111000\n",
      "nearby: doc_list: 11110110\n",
      "near: doc_list: 111000\n",
      "necessary: doc_list: 111001\n",
      "necessarywell: doc_list: 11110110\n",
      "negotiations: doc_list: 111001\n",
      "neighbor: doc_list: 10111011\n",
      "neighborhood: doc_list: 10111010\n",
      "neighbors: doc_list: 11110000111000\n",
      "never: doc_list: 10\n",
      "nevertheless: doc_list: 1101\n",
      "newest: doc_list: 11110010\n",
      "new: doc_list: 1101101110001010\n",
      "newspaper: doc_list: 10111010\n",
      "newsstand: doc_list: 111010\n",
      "next: doc_list: 1011110010101100\n",
      "nextdoor: doc_list: 10\n",
      "nice: doc_list: 10110010111001\n",
      "nicest: doc_list: 10\n",
      "night: doc_list: 1011110011\n",
      "nightit: doc_list: 10\n",
      "nine: doc_list: 11110011\n",
      "nitrogen: doc_list: 1101\n",
      "north: doc_list: 1110101101\n",
      "nobody: doc_list: 10\n",
      "noise: doc_list: 11110100\n",
      "nonfat: doc_list: 11110101\n",
      "nonflammable: doc_list: 11110110\n",
      "northville: doc_list: 11110010\n",
      "notify: doc_list: 1101\n",
      "number: doc_list: 1100\n",
      "nutritious: doc_list: 11110101\n",
      "numbers: doc_list: 1100\n",
      "occurred: doc_list: 101011110001\n",
      "oceanside: doc_list: 11110100\n",
      "occurs: doc_list: 10\n",
      "octane: doc_list: 111011\n",
      "offer: doc_list: 111000\n",
      "offers: doc_list: 111000\n",
      "officer: doc_list: 111010\n",
      "officers: doc_list: 11100110\n",
      "official: doc_list: 11110100\n",
      "officials: doc_list: 111101001100\n",
      "offset: doc_list: 11110100\n",
      "often: doc_list: 111011\n",
      "oil: doc_list: 11110000\n",
      "ok: doc_list: 111011\n",
      "officials: doc_list: 11100010\n",
      "old: doc_list: 1100111011111000\n",
      "one: doc_list: 1010101100110011001010101100\n",
      "onehour: doc_list: 111010\n",
      "onebedroom: doc_list: 111000\n",
      "ones: doc_list: 1111000010\n",
      "onto: doc_list: 111010\n",
      "operationninety: doc_list: 11110110\n",
      "opportunityeveryone: doc_list: 111000\n",
      "order: doc_list: 111001\n",
      "others: doc_list: 11110100\n",
      "ought: doc_list: 11110110\n",
      "ordering: doc_list: 111010111000\n",
      "outdoor: doc_list: 10\n",
      "outdoors: doc_list: 10\n",
      "outside: doc_list: 111010\n",
      "overpaid: doc_list: 111000\n",
      "overpaying: doc_list: 111000\n",
      "owners: doc_list: 11110100\n",
      "owns: doc_list: 11110101\n",
      "owner: doc_list: 1100110011110000\n",
      "paid: doc_list: 1110001111000110\n",
      "painmrs: doc_list: 10\n",
      "paper: doc_list: 111000\n",
      "park: doc_list: 11110100\n",
      "paperthe: doc_list: 111001\n",
      "parked: doc_list: 111010\n",
      "parking: doc_list: 1011110011\n",
      "party: doc_list: 11110100\n",
      "pasadena: doc_list: 11110101\n",
      "part: doc_list: 11110100\n",
      "passing: doc_list: 1110001101\n",
      "pay: doc_list: 1100101011110010\n",
      "paying: doc_list: 111011\n",
      "people: doc_list: 111101001100\n",
      "penny: doc_list: 111011\n",
      "per: doc_list: 111011111001\n",
      "people: doc_list: 101011101010\n",
      "percent: doc_list: 101110001011001011011100\n",
      "perfect: doc_list: 1101\n",
      "personal: doc_list: 11110110\n",
      "permitted: doc_list: 11110000\n",
      "persons: doc_list: 11110000\n",
      "period: doc_list: 101100\n",
      "phoenix: doc_list: 1101\n",
      "pianist: doc_list: 11110000\n",
      "phone: doc_list: 1100\n",
      "piano: doc_list: 1100111010\n",
      "pile: doc_list: 11110100\n",
      "pine: doc_list: 11110110\n",
      "picked: doc_list: 1100111010\n",
      "pistol: doc_list: 11110001\n",
      "planners: doc_list: 11110110\n",
      "plants: doc_list: 11110110\n",
      "pizza: doc_list: 11110001\n",
      "plates: doc_list: 111001\n",
      "place: doc_list: 101101111001\n",
      "plastic: doc_list: 11110011\n",
      "play: doc_list: 1100\n",
      "played: doc_list: 11110000\n",
      "player: doc_list: 11110000\n",
      "playgrounda: doc_list: 11110110\n",
      "playing: doc_list: 11110000\n",
      "plowed: doc_list: 111010\n",
      "plus: doc_list: 11110101\n",
      "pm: doc_list: 11110010\n",
      "plot: doc_list: 11110001\n",
      "police: doc_list: 101110011010110110\n",
      "popular: doc_list: 10111001\n",
      "portable: doc_list: 11110011\n",
      "post: doc_list: 11110100\n",
      "practice: doc_list: 11110001\n",
      "practiced: doc_list: 11110000\n",
      "pretty: doc_list: 111011111011\n",
      "president: doc_list: 1101\n",
      "price: doc_list: 1101101101111010\n",
      "prince: doc_list: 11110010\n",
      "prices: doc_list: 101110001101111010\n",
      "prison: doc_list: 111001\n",
      "private: doc_list: 11110110\n",
      "prisoners: doc_list: 111001\n",
      "probably: doc_list: 11110010\n",
      "problem: doc_list: 111100101100\n",
      "problems: doc_list: 111101001100\n",
      "proceeds: doc_list: 11110011\n",
      "produce: doc_list: 110111110001\n",
      "profit: doc_list: 111000\n",
      "property: doc_list: 11110110\n",
      "proposal: doc_list: 11110100\n",
      "proprietor: doc_list: 111010\n",
      "pros: doc_list: 11110100\n",
      "proven: doc_list: 11110100\n",
      "provide: doc_list: 111001\n",
      "pulp: doc_list: 11110001\n",
      "pumping: doc_list: 111011\n",
      "pursuing: doc_list: 111010\n",
      "put: doc_list: 11101011110000\n",
      "puppies: doc_list: 1101\n",
      "quell: doc_list: 111001\n",
      "question: doc_list: 111000\n",
      "questionandanswer: doc_list: 10\n",
      "quieter: doc_list: 10\n",
      "quite: doc_list: 11110010\n",
      "radios: doc_list: 11110011\n",
      "raging: doc_list: 11110110\n",
      "rain: doc_list: 11110011\n",
      "quietershe: doc_list: 10\n",
      "rainstorm: doc_list: 11110110\n",
      "raining: doc_list: 11110001\n",
      "raises: doc_list: 11110000\n",
      "ran: doc_list: 111010\n",
      "raffle: doc_list: 1100\n",
      "range: doc_list: 11110110\n",
      "rates: doc_list: 111000\n",
      "rather: doc_list: 111011\n",
      "ration: doc_list: 111001\n",
      "rattle: doc_list: 11110001\n",
      "razors: doc_list: 111001\n",
      "reading: doc_list: 10\n",
      "reads: doc_list: 10\n",
      "ready: doc_list: 1101\n",
      "reason: doc_list: 11101110\n",
      "realtor: doc_list: 111000\n",
      "recent: doc_list: 11110110\n",
      "reconsider: doc_list: 11110100\n",
      "receives: doc_list: 1101\n",
      "reduce: doc_list: 111001\n",
      "reduced: doc_list: 111011\n",
      "refill: doc_list: 11110010\n",
      "refused: doc_list: 11110010\n",
      "regular: doc_list: 11110101\n",
      "reduction: doc_list: 111001\n",
      "released: doc_list: 111001\n",
      "remaining: doc_list: 1101\n",
      "remarked: doc_list: 111000\n",
      "remove: doc_list: 11110110\n",
      "removed: doc_list: 11110110\n",
      "rent: doc_list: 11110000111000\n",
      "rental: doc_list: 11110100\n",
      "rentals: doc_list: 11110100\n",
      "renters: doc_list: 11110000111000\n",
      "renting: doc_list: 11110000\n",
      "rents: doc_list: 10\n",
      "repairs: doc_list: 1100\n",
      "residents: doc_list: 111101001100\n",
      "respond: doc_list: 11110100\n",
      "response: doc_list: 11110100\n",
      "rest: doc_list: 11110011\n",
      "resident: doc_list: 1111010110\n",
      "restaurants: doc_list: 11110010\n",
      "restaurant: doc_list: 1111000110\n",
      "restored: doc_list: 111001\n",
      "resulted: doc_list: 11110100\n",
      "resumed: doc_list: 111010\n",
      "residents: doc_list: 10111011\n",
      "retired: doc_list: 10\n",
      "retirees: doc_list: 11110011\n",
      "revolver: doc_list: 11110001\n",
      "return: doc_list: 111001\n",
      "rick: doc_list: 11110100\n",
      "right: doc_list: 11001110011100\n",
      "roadside: doc_list: 11110011\n",
      "road: doc_list: 111000\n",
      "roof: doc_list: 11110000\n",
      "room: doc_list: 111001\n",
      "ruined: doc_list: 111010\n",
      "run: doc_list: 11001100110010\n",
      "safely: doc_list: 11110110\n",
      "sad: doc_list: 10\n",
      "said: doc_list: 1010101010101100101010101100\n",
      "sale: doc_list: 11100011110001\n",
      "sam: doc_list: 1100\n",
      "samantha: doc_list: 11110000\n",
      "san: doc_list: 10\n",
      "sandwich: doc_list: 11110010\n",
      "sandwiches: doc_list: 10\n",
      "sara: doc_list: 11110101\n",
      "saturday: doc_list: 1111001010\n",
      "santa: doc_list: 111000\n",
      "save: doc_list: 111011\n",
      "saved: doc_list: 11110001\n",
      "savings: doc_list: 111011\n",
      "saturday: doc_list: 101111000010\n",
      "saxophone: doc_list: 11110000\n",
      "saxophonist: doc_list: 11110000\n",
      "saw: doc_list: 111000111000\n",
      "say: doc_list: 101101111001111001\n",
      "saying: doc_list: 11110010\n",
      "says: doc_list: 11001010\n",
      "scaldinghe: doc_list: 11110010\n",
      "scouts: doc_list: 11110011\n",
      "scheduled: doc_list: 11110011\n",
      "seashell: doc_list: 111011\n",
      "seatbelt: doc_list: 111010\n",
      "scared: doc_list: 10\n",
      "second: doc_list: 1100\n",
      "secured: doc_list: 111001\n",
      "security: doc_list: 11110110\n",
      "seekers: doc_list: 10\n",
      "seem: doc_list: 1101111001\n",
      "seemed: doc_list: 10\n",
      "selected: doc_list: 1100\n",
      "sell: doc_list: 111000111011\n",
      "sellers: doc_list: 111000\n",
      "several: doc_list: 11101010\n",
      "shaking: doc_list: 111010\n",
      "sent: doc_list: 11011100\n",
      "set: doc_list: 11110011\n",
      "seven: doc_list: 11110101\n",
      "shapes: doc_list: 11110011\n",
      "sherman: doc_list: 11110010\n",
      "shiny: doc_list: 11110011\n",
      "shopping: doc_list: 111100111100\n",
      "shave: doc_list: 10\n",
      "show: doc_list: 111011\n",
      "shorter: doc_list: 10\n",
      "sicker: doc_list: 10\n",
      "side: doc_list: 111010\n",
      "sickness: doc_list: 10\n",
      "sidewalksnoise: doc_list: 11110100\n",
      "sign: doc_list: 1100\n",
      "silverware: doc_list: 111001\n",
      "since: doc_list: 11110101\n",
      "six: doc_list: 110011110100\n",
      "sixth: doc_list: 10\n",
      "sixtyyearold: doc_list: 1100\n",
      "sizes: doc_list: 11110011\n",
      "slacks: doc_list: 11110010\n",
      "skyrocketing: doc_list: 111011\n",
      "slam: doc_list: 111010\n",
      "sleeping: doc_list: 11110101\n",
      "slightly: doc_list: 11110010\n",
      "slowly: doc_list: 1100\n",
      "slumped: doc_list: 111010\n",
      "smiled: doc_list: 11110011\n",
      "smith: doc_list: 11110101\n",
      "soap: doc_list: 111001\n",
      "sneaking: doc_list: 10\n",
      "social: doc_list: 11110110\n",
      "sofas: doc_list: 11110011\n",
      "something: doc_list: 11110010\n",
      "sold: doc_list: 10111000\n",
      "sometime: doc_list: 111000\n",
      "sometimes: doc_list: 11110100\n",
      "sought: doc_list: 10\n",
      "southern: doc_list: 111011\n",
      "southland: doc_list: 111011\n",
      "space: doc_list: 10\n",
      "spanish: doc_list: 11110001\n",
      "specter: doc_list: 11110011\n",
      "sparing: doc_list: 111001\n",
      "spewed: doc_list: 111010\n",
      "spill: doc_list: 11110010\n",
      "spilled: doc_list: 11110010\n",
      "spinning: doc_list: 1100\n",
      "sponsored: doc_list: 10\n",
      "sports: doc_list: 1100\n",
      "stain: doc_list: 11110010\n",
      "spring: doc_list: 111010\n",
      "stand: doc_list: 111010\n",
      "standing: doc_list: 111010\n",
      "stars: doc_list: 11110001\n",
      "state: doc_list: 11110110\n",
      "started: doc_list: 111010\n",
      "station: doc_list: 111011\n",
      "stationery: doc_list: 111001\n",
      "state: doc_list: 1100110010\n",
      "stay: doc_list: 111000\n",
      "steadily: doc_list: 10\n",
      "steering: doc_list: 111010\n",
      "steep: doc_list: 1101\n",
      "still: doc_list: 110011001111000010\n",
      "stopped: doc_list: 1111010010\n",
      "stolen: doc_list: 111010\n",
      "store: doc_list: 1110111100111000\n",
      "stop: doc_list: 1100111000\n",
      "stored: doc_list: 1101\n",
      "stray: doc_list: 1101\n",
      "streambed: doc_list: 11110011\n",
      "streets: doc_list: 11110100\n",
      "stretch: doc_list: 11110011\n",
      "street: doc_list: 1100111000111011\n",
      "studio: doc_list: 1100\n",
      "succeed: doc_list: 10\n",
      "successfully: doc_list: 1101\n",
      "sudden: doc_list: 11110110\n",
      "sue: doc_list: 11110010\n",
      "sued: doc_list: 1100\n",
      "suggested: doc_list: 11110100\n",
      "summer: doc_list: 11110100\n",
      "supposed: doc_list: 11110110\n",
      "surcharge: doc_list: 11110100\n",
      "sure: doc_list: 111101001100\n",
      "surround: doc_list: 11110110\n",
      "surrounded: doc_list: 11110110\n",
      "survive: doc_list: 11110110\n",
      "suffered: doc_list: 11110010\n",
      "suv: doc_list: 111010\n",
      "sunday: doc_list: 10\n",
      "table: doc_list: 111000\n",
      "take: doc_list: 11110011\n",
      "taken: doc_list: 111010\n",
      "talk: doc_list: 1010\n",
      "talked: doc_list: 10\n",
      "talking: doc_list: 1010\n",
      "tank: doc_list: 111011\n",
      "talks: doc_list: 10\n",
      "tax: doc_list: 11110101\n",
      "taxes: doc_list: 1100\n",
      "teen: doc_list: 1100\n",
      "teenage: doc_list: 1100\n",
      "teenager: doc_list: 1100\n",
      "tell: doc_list: 101110101100\n",
      "telling: doc_list: 111000\n",
      "thats: doc_list: 1110001101\n",
      "theater: doc_list: 11110001\n",
      "thelma: doc_list: 11110110\n",
      "themsome: doc_list: 11110100\n",
      "theyd: doc_list: 111011\n",
      "theres: doc_list: 11100011110010\n",
      "thick: doc_list: 11110110\n",
      "thing: doc_list: 1100\n",
      "things: doc_list: 1011011100\n",
      "think: doc_list: 111101001100\n",
      "thinks: doc_list: 11110100\n",
      "though: doc_list: 111011\n",
      "three: doc_list: 1110101101\n",
      "thought: doc_list: 111000111011\n",
      "three: doc_list: 111100111010\n",
      "threebedroom: doc_list: 11110100\n",
      "thousands: doc_list: 11110110\n",
      "threehour: doc_list: 1100\n",
      "threw: doc_list: 11100110\n",
      "tim: doc_list: 111000\n",
      "time: doc_list: 1011001110101100\n",
      "timethe: doc_list: 11110011\n",
      "times: doc_list: 11001110011100\n",
      "tired: doc_list: 111000111000\n",
      "tires: doc_list: 11110011\n",
      "toaster: doc_list: 11110101\n",
      "today: doc_list: 111011\n",
      "todays: doc_list: 111011\n",
      "together: doc_list: 10\n",
      "toilet: doc_list: 111001\n",
      "told: doc_list: 111000111000\n",
      "tons: doc_list: 10111100101101\n",
      "took: doc_list: 11110001\n",
      "toothe: doc_list: 10\n",
      "toppled: doc_list: 11110110\n",
      "toss: doc_list: 11110100\n",
      "totally: doc_list: 11110100\n",
      "toward: doc_list: 11110110\n",
      "town: doc_list: 11110110\n",
      "toyola: doc_list: 11110101\n",
      "tower: doc_list: 111001\n",
      "trade: doc_list: 111001\n",
      "tough: doc_list: 1100\n",
      "trash: doc_list: 1111001110\n",
      "tree: doc_list: 11110110\n",
      "trees: doc_list: 11110110\n",
      "traffic: doc_list: 10111011111010\n",
      "tried: doc_list: 11100110\n",
      "triplescoop: doc_list: 11110011\n",
      "try: doc_list: 11110110\n",
      "trucks: doc_list: 111100111101\n",
      "trying: doc_list: 111011\n",
      "tuner: doc_list: 1100\n",
      "tuneups: doc_list: 11110000\n",
      "turned: doc_list: 111010\n",
      "tv: doc_list: 10110011110000\n",
      "twenty: doc_list: 111000\n",
      "twice: doc_list: 10110011110011\n",
      "twin: doc_list: 1101\n",
      "two: doc_list: 10101101101010110010101010\n",
      "twobedroom: doc_list: 111000\n",
      "twomile: doc_list: 11110011\n",
      "underbrush: doc_list: 11110110\n",
      "unemployed: doc_list: 1100\n",
      "unfortunately: doc_list: 11110110\n",
      "unhappy: doc_list: 11110000\n",
      "uninjured: doc_list: 111010\n",
      "uneventful: doc_list: 111010\n",
      "unlucky: doc_list: 1100\n",
      "unpredictable: doc_list: 10\n",
      "upset: doc_list: 11110010\n",
      "use: doc_list: 11110011\n",
      "us: doc_list: 10\n",
      "used: doc_list: 101110101100\n",
      "value: doc_list: 11110011\n",
      "variable: doc_list: 1100\n",
      "variables: doc_list: 1100\n",
      "vehicles: doc_list: 111011\n",
      "vehiclewhen: doc_list: 111010\n",
      "vendor: doc_list: 111010\n",
      "venice: doc_list: 111000\n",
      "veterinarian: doc_list: 1101\n",
      "vicky: doc_list: 10\n",
      "victoria: doc_list: 10\n",
      "video: doc_list: 11110001\n",
      "videos: doc_list: 1011110001\n",
      "visiting: doc_list: 111000\n",
      "visitors: doc_list: 11110100\n",
      "vittorio: doc_list: 10\n",
      "volunteers: doc_list: 11110011\n",
      "vittorios: doc_list: 10\n",
      "wait: doc_list: 101101111000\n",
      "waiting: doc_list: 11110010\n",
      "waited: doc_list: 111011\n",
      "walked: doc_list: 11110001\n",
      "walks: doc_list: 11110001\n",
      "warden: doc_list: 111001\n",
      "want: doc_list: 111000\n",
      "wash: doc_list: 11110010\n",
      "washes: doc_list: 10\n",
      "wasnt: doc_list: 111011\n",
      "watch: doc_list: 111100011100\n",
      "watches: doc_list: 10\n",
      "watching: doc_list: 111000\n",
      "way: doc_list: 1111010110\n",
      "water: doc_list: 111010\n",
      "weaned: doc_list: 1101\n",
      "wearing: doc_list: 111010\n",
      "website: doc_list: 11110100\n",
      "week: doc_list: 10111000111011\n",
      "weekend: doc_list: 1011110000\n",
      "weekly: doc_list: 11110100\n",
      "weeks: doc_list: 1101\n",
      "well: doc_list: 11011110101100\n",
      "went: doc_list: 111001101101111000\n",
      "whales: doc_list: 11110011\n",
      "west: doc_list: 10111010\n",
      "whatever: doc_list: 111000\n",
      "wheel: doc_list: 111010\n",
      "wheels: doc_list: 111011\n",
      "whether: doc_list: 11100011110000\n",
      "whiskey: doc_list: 111010\n",
      "white: doc_list: 11110001\n",
      "whole: doc_list: 11110000\n",
      "whose: doc_list: 1101\n",
      "widow: doc_list: 11110110\n",
      "widowed: doc_list: 10\n",
      "wife: doc_list: 10\n",
      "win: doc_list: 1100\n",
      "windows: doc_list: 111001\n",
      "winnings: doc_list: 1100\n",
      "woke: doc_list: 11110101\n",
      "woman: doc_list: 10\n",
      "wont: doc_list: 111100111101\n",
      "worked: doc_list: 11110011\n",
      "work: doc_list: 11101111001100101100\n",
      "world: doc_list: 10111011\n",
      "worms: doc_list: 11110100\n",
      "worth: doc_list: 1100110011101110\n",
      "would: doc_list: 10101011011101101010\n",
      "wrong: doc_list: 111000111000\n",
      "years: doc_list: 11110011101010\n",
      "year: doc_list: 1010110010111000\n",
      "yelling: doc_list: 111010\n",
      "yellow: doc_list: 11110011\n",
      "yet: doc_list: 11110000\n",
      "yield: doc_list: 111010\n",
      "young: doc_list: 1110111101\n",
      "years: doc_list: 101010110011011100\n"
     ]
    }
   ],
   "source": [
    "for token in inverted_index.posting_list:\n",
    "    print(f\"{token.word}: doc_list: {token.docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Testing\n",
    "Now we can test and query to our system. Note that the documents are 0-based indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T13:07:42.968360Z",
     "end_time": "2023-11-23T13:07:43.023696Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[11, 13, 14]\n",
      "[11, 13, 14]\n",
      "[10, 7]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12]\n"
     ]
    }
   ],
   "source": [
    "ir_system = QueryProcessor(inverted_index)\n",
    "# # Now we test our sample\n",
    "print(ir_system.search('another'))\n",
    "print(ir_system.search('back or another'))\n",
    "print(ir_system.search('back'))\n",
    "print(ir_system.search('young'))\n",
    "print(ir_system.search('not back'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
